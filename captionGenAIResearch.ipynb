{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "captionGenAIResearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshita21/Image-summarization-using-hybrid-CNN--LSTM-networks-and-corresponding-Speech-Synthesis-using-Mel-Spe/blob/main/captionGenAIResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZGkgxI180q-"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import glob\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "import tensorflow.keras.applications.mobilenet  \n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "import tensorflow.keras.applications.inception_v3\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tensorflow.keras.preprocessing.image\n",
        "import pickle\n",
        "from time import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "START = \"startseq\"\n",
        "STOP = \"endseq\"\n",
        "EPOCHS = 10\n",
        "USE_INCEPTION = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjUSp7B9843P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b247f8-3c0b-46ed-c238-e00e2e0c53b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge-gVW_6-uSs"
      },
      "source": [
        "!cat '/content/drive/My Drive/ML Links/caption_link.txt'\n",
        "!wget -i '/content/drive/My Drive/ML Links/caption_link.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peLvLWlSACcC"
      },
      "source": [
        "!unzip glove.6B.zip -d glove.6B/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efil7ZXWC_Wj"
      },
      "source": [
        "!unzip Flickr8k_Dataset.zip -d Flickr8k_Dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSz44GwvDyxe"
      },
      "source": [
        "!unzip Flickr8k_text.zip -d Flickr8k_text/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBG6K8v7EkJD"
      },
      "source": [
        "null_punct = str.maketrans('', '', string.punctuation)\n",
        "lookup = dict()\n",
        "\n",
        "with open( os.path.join('Flickr8k_text','Flickr8k.token.txt'), 'r') as fp:\n",
        "  \n",
        "  max_length = 0\n",
        "  for line in fp.read().split('\\n'):\n",
        "    tok = line.split()\n",
        "    if len(line) >= 2:\n",
        "      id = tok[0].split('.')[0]\n",
        "      desc = tok[1:]\n",
        "      \n",
        "      # Cleanup description\n",
        "      desc = [word.lower() for word in desc]\n",
        "      desc = [w.translate(null_punct) for w in desc]\n",
        "      desc = [word for word in desc if len(word)>1]\n",
        "      desc = [word for word in desc if word.isalpha()]\n",
        "      max_length = max(max_length,len(desc))\n",
        "      \n",
        "      if id not in lookup:\n",
        "        lookup[id] = list()\n",
        "      lookup[id].append(' '.join(desc))\n",
        "      \n",
        "lex = set()\n",
        "for key in lookup:\n",
        "  [lex.update(d.split()) for d in lookup[key]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuhROXonFCWP"
      },
      "source": [
        "print(len(lookup)) # How many unique words\n",
        "print(len(lex)) # The dictionary\n",
        "print(max_length) # Maximum length of a caption (in words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeI4pZ4cF3Rt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "plt.title(\"Count Plot\")\n",
        "plt.xlabel(\"vales\")\n",
        "plt.xticks(np.arange(0,1600,100),rotation=90)\n",
        "plt.ylabel('counts')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ5CIZQ6FyDf"
      },
      "source": [
        "img = glob.glob(os.path.join('Flickr8k_Dataset','Flicker8k_Dataset', '*.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqyMbG5ZGHvE"
      },
      "source": [
        "len(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L0ensJXGMLA"
      },
      "source": [
        "train_images_path = os.path.join('Flickr8k_text','Flickr_8k.trainImages.txt') \n",
        "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
        "test_images_path = os.path.join('Flickr8k_text','Flickr_8k.testImages.txt') \n",
        "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
        "\n",
        "train_img = []\n",
        "test_img = []\n",
        "\n",
        "for i in img:\n",
        "  f = os.path.split(i)[-1]\n",
        "  if f in train_images: \n",
        "    train_img.append(f) \n",
        "  elif f in test_images:\n",
        "    test_img.append(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA_5pph4G8oJ"
      },
      "source": [
        "\n",
        "print(len(train_images))\n",
        "print(len(test_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khElMfosHF23"
      },
      "source": [
        "train_descriptions = {k:v for k,v in lookup.items() if f'{k}.jpg' in train_images}\n",
        "for n,v in train_descriptions.items(): \n",
        "  for d in range(len(v)):\n",
        "    v[d] = f'{START} {v[d]} {STOP}'\n",
        "\n",
        "len(train_descriptions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_RNyxcHNR_"
      },
      "source": [
        "if USE_INCEPTION:\n",
        "  encode_model = InceptionV3(weights='imagenet')\n",
        "  encode_model = Model(encode_model.input, encode_model.layers[-2].output)\n",
        "  WIDTH = 299\n",
        "  HEIGHT = 299\n",
        "  OUTPUT_DIM = 2048\n",
        "  preprocess_input = tensorflow.keras.applications.inception_v3.preprocess_input\n",
        "else:\n",
        "  encode_model = MobileNet(weights='imagenet',include_top=False)\n",
        "  WIDTH = 224\n",
        "  HEIGHT = 224\n",
        "  OUTPUT_DIM = 50176\n",
        "  preprocess_input = tensorflow.keras.applications.mobilenet.preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g5lwXtaHXpv"
      },
      "source": [
        "encode_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgYg7EKdHdqt"
      },
      "source": [
        "\n",
        "def encodeImage(img):\n",
        "  # Resize all images to a standard size (specified bythe image encoding network)\n",
        "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
        "  # Convert a PIL image to a numpy array\n",
        "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
        "  # Expand to 2D array\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  # Perform any preprocessing needed by InceptionV3 or others\n",
        "  x = preprocess_input(x)\n",
        "  # Call InceptionV3 (or other) to extract the smaller feature set for the image.\n",
        "  x = encode_model.predict(x) # Get the encoding vector for the image\n",
        "  # Shape to correct form to be accepted by LSTM captioning network.\n",
        "  x = np.reshape(x, OUTPUT_DIM )\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjcNbk5ZH_VY"
      },
      "source": [
        "train_path = os.path.join(\"data\",f'train{OUTPUT_DIM}.pkl')\n",
        "if not os.path.exists(train_path):\n",
        "  start = time()\n",
        "  encoding_train = {}\n",
        "  for id in tqdm(train_img):\n",
        "    image_path = os.path.join('Flickr8k_Dataset','Flicker8k_Dataset', id)\n",
        "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
        "    encoding_train[id] = encodeImage(img)\n",
        "  with open(train_path, \"wb\") as fp:\n",
        "    pickle.dump(encoding_train, fp)\n",
        "else:\n",
        "  with open(train_path, \"rb\") as fp:\n",
        "    encoding_train = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfKUzKQ3IEkS"
      },
      "source": [
        "test_path = os.path.join(\"data\",f'test{OUTPUT_DIM}.pkl')\n",
        "if not os.path.exists(test_path):\n",
        "  start = time()\n",
        "  encoding_test = {}\n",
        "  for id in tqdm(test_img):\n",
        "    image_path = os.path.join('Flickr8k_Dataset','Flicker8k_Dataset', id)\n",
        "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
        "    encoding_test[id] = encodeImage(img)\n",
        "  with open(test_path, \"wb\") as fp:\n",
        "    pickle.dump(encoding_test, fp)\n",
        "else:\n",
        "  with open(test_path, \"rb\") as fp:\n",
        "    encoding_test = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQVqwBeGSJQQ"
      },
      "source": [
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)\n",
        "len(all_train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_obm-40yTwHn"
      },
      "source": [
        "\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhnHWce7TzoJ"
      },
      "source": [
        "\n",
        "idxtoword = {}\n",
        "wordtoidx = {}\n",
        "\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoidx[w] = ix\n",
        "    idxtoword[ix] = w\n",
        "    ix += 1\n",
        "    \n",
        "vocab_size = len(idxtoword) + 1 \n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67oxiljT26T"
      },
      "source": [
        "max_length +=2\n",
        "print(max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zQPrrntT58m"
      },
      "source": [
        "def data_generator(descriptions, photos, wordtoidx, max_length, num_photos_per_batch):\n",
        "  # x1 - Training data for photos\n",
        "  # x2 - The caption that goes with each photo\n",
        "  # y - The predicted rest of the caption\n",
        "  x1, x2, y = [], [], []\n",
        "  n=0\n",
        "  while True:\n",
        "    for key, desc_list in descriptions.items():\n",
        "      n+=1\n",
        "      photo = photos[key+'.jpg']\n",
        "      # Each photo has 5 descriptions\n",
        "      for desc in desc_list:\n",
        "        # Convert each word into a list of sequences.\n",
        "        seq = [wordtoidx[word] for word in desc.split(' ') if word in wordtoidx]\n",
        "        # Generate a training case for every possible sequence and outcome\n",
        "        for i in range(1, len(seq)):\n",
        "          in_seq, out_seq = seq[:i], seq[i]\n",
        "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "          x1.append(photo)\n",
        "          x2.append(in_seq)\n",
        "          y.append(out_seq)\n",
        "      if n==num_photos_per_batch:\n",
        "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
        "        x1, x2, y = [], [], []\n",
        "        n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXuCmGbmUBrf"
      },
      "source": [
        "\n",
        "glove_dir = os.path.join('glove.6B')\n",
        "embeddings_index = {} \n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsCufffVUGsD"
      },
      "source": [
        "embedding_dim = 200\n",
        "\n",
        "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in wordtoidx.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "\n",
        "embedding_matrix.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFgaFC6WUSwV"
      },
      "source": [
        "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi3RUxcbUfBg"
      },
      "source": [
        "caption_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEEzYkUQUh78"
      },
      "source": [
        "caption_model.layers[2].set_weights([embedding_matrix])\n",
        "caption_model.layers[2].trainable = False\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE5Q-LKtUrBb"
      },
      "source": [
        "number_pics_per_bath = 3\n",
        "steps = len(train_descriptions)//number_pics_per_bath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcHdFtE1UutG"
      },
      "source": [
        "model_path = os.path.join(\"data\",f'caption-model.hdf5')\n",
        "if not os.path.exists(model_path):\n",
        "  for i in tqdm(range(EPOCHS*2)):\n",
        "      generator = data_generator(train_descriptions, encoding_train, wordtoidx, max_length, number_pics_per_bath)\n",
        "      caption_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "  caption_model.optimizer.lr = 1e-4\n",
        "  number_pics_per_bath = 6\n",
        "  steps = len(train_descriptions)//number_pics_per_bath\n",
        "\n",
        "  for i in range(EPOCHS):\n",
        "      generator = data_generator(train_descriptions, encoding_train, wordtoidx, max_length, number_pics_per_bath)\n",
        "      caption_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)  \n",
        "  caption_model.save_weights(model_path)\n",
        "else:\n",
        "  caption_model.load_weights(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7nTr3g9U0-Q"
      },
      "source": [
        "def generateCaption(photo):\n",
        "    in_text = START\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = caption_model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = idxtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == STOP:\n",
        "            break\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR95DqOU6KM6"
      },
      "source": [
        "\n",
        "for z in range(10):\n",
        "  pic = list(encoding_test.keys())[z]\n",
        "  image = encoding_test[pic].reshape((1,OUTPUT_DIM))\n",
        "  print(os.path.join('Flickr8k_Dataset','Flicker8k_Dataset', pic))\n",
        "  x=plt.imread(os.path.join('Flickr8k_Dataset','Flicker8k_Dataset', pic))\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n",
        "  print(\"Caption:\",generateCaption(image))\n",
        "  print(\"_____________________________________\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEHXPeSz6Nyr"
      },
      "source": [
        "sample_gen_captions=[\"boy in red shirt is jumping off of slide\", \"man in black shirt and jeans is sitting on bench in front of building\",\"young boy wearing red shirt and blue shorts kicking soccer ball\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csqzNGyvzpA_"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, expanduser\n",
        "\n",
        "os.chdir(expanduser(\"~\"))\n",
        "\n",
        "wavenet_dir = \"wavenet_vocoder\"\n",
        "if not exists(wavenet_dir):\n",
        "  ! git clone https://github.com/r9y9/$wavenet_dir\n",
        "  ! cd wavenet_vocoder && git checkout v0.1.1 && cd -\n",
        "    \n",
        "taco2_dir = \"Tacotron-2\"\n",
        "if not exists(taco2_dir):\n",
        "  ! git clone https://github.com/r9y9/$taco2_dir\n",
        "  ! cd $taco2_dir && git checkout -B wavenet3 origin/wavenet3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6L45OeDf7G5"
      },
      "source": [
        "os.chdir(join(expanduser(\"~\"), taco2_dir))\n",
        "text_file = open(\"text_list.txt\", \"w\")\n",
        "n = text_file.write(sample_gen_captions[1])\n",
        "text_file.close()\n",
        "\n",
        "text_file = open(\"text_list.txt\", \"r\")\n",
        "sent=text_file.read()\n",
        "text_file.close()\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5zdTgZru28Z"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, expanduser\n",
        "\n",
        "\n",
        "wavenet_dir = \"wavenet_vocoder\"\n",
        "if not exists(wavenet_dir):\n",
        "  ! git clone https://github.com/r9y9/$wavenet_dir\n",
        "  ! cd wavenet_vocoder && git checkout v0.1.1 && cd -\n",
        "    \n",
        "taco2_dir = \"Tacotron-2\"\n",
        "if not exists(taco2_dir):\n",
        "  ! git clone https://github.com/r9y9/$taco2_dir\n",
        "  ! cd $taco2_dir && git checkout -B wavenet3 origin/wavenet3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jyJygjEu5vh"
      },
      "source": [
        "# Install dependencies\n",
        "! pip install -q -U \"tensorflow<=1.9.0\"\n",
        "! pip install -q -U \"keras==2.2.4\"\n",
        "! pip install -q -U \"numpy<1.16\"\n",
        "! pip install -q -U \"pysptk<=0.1.14\"\n",
        "\n",
        "os.chdir(join(expanduser(\"~\"), taco2_dir))\n",
        "! pip install -q -r requirements.txt\n",
        "\n",
        "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
        "! pip install -q -e '.[train]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccgcIX2xvSH9"
      },
      "source": [
        "import torch\n",
        "import tensorflow\n",
        "import pysptk\n",
        "import numpy as np\n",
        "tensorflow.__version__, pysptk.__version__, np.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEmqWAj3vgc1"
      },
      "source": [
        "os.chdir(join(expanduser(\"~\"), taco2_dir))\n",
        "! mkdir -p logs-Tacotron\n",
        "if not exists(\"logs-Tacotron/pretrained\"):\n",
        "  ! curl -O -L \"https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz\"\n",
        "  ! tar xzvf pretrained.tar.gz\n",
        "  ! mv pretrained logs-Tacotron"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_aUZsDwvvAM"
      },
      "source": [
        "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
        "wn_preset = \"20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
        "wn_checkpoint_path = \"20180510_mixture_lj_checkpoint_step000320000_ema.pth\"\n",
        "\n",
        "if not exists(wn_preset):\n",
        "  !curl -O -L \"https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
        "if not exists(wn_checkpoint_path):\n",
        "  !curl -O -L \"https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QQXBkLdv4b4"
      },
      "source": [
        "os.chdir(join(expanduser(\"~\"), taco2_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzeh08tWwmW0"
      },
      "source": [
        "# Remove old files if exist\n",
        "! rm -rf tacotron_output\n",
        "! python synthesize.py --model='Tacotron' --mode='eval' \\\n",
        "  --hparams='symmetric_mels=False,max_abs_value=4.0,power=1.1,outputs_per_step=1' \\\n",
        "  --text_list=./text_list.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64TQjUQPytfd"
      },
      "source": [
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imXb4_NkyzP6"
      },
      "source": [
        "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
        "\n",
        "# Setup WaveNet vocoder hparams\n",
        "from hparams import hparams\n",
        "with open(wn_preset) as f:\n",
        "    hparams.parse_json(f.read())\n",
        "\n",
        "# Setup WaveNet vocoder\n",
        "from train import build_model\n",
        "from synthesis import wavegen\n",
        "import torch\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "model = build_model().to(device)\n",
        "\n",
        "print(\"Load checkpoint from {}\".format(wn_checkpoint_path))\n",
        "checkpoint = torch.load(wn_checkpoint_path)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILUYPnXZy1OE"
      },
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(\"../Tacotron-2/tacotron_output/eval/map.txt\") as f:\n",
        "  maps = f.readlines()\n",
        "maps = list(map(lambda x:x[:-1].split(\"|\"), maps))\n",
        "# filter out invalid ones\n",
        "maps = list(filter(lambda x:len(x) == 2, maps))\n",
        "\n",
        "print(\"List of texts to be synthesized\")\n",
        "for idx, (text,_) in enumerate(maps):\n",
        "  print(idx, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qGf81yQ1Fib"
      },
      "source": [
        "waveforms = []\n",
        "\n",
        "for idx, (text, mel) in enumerate(maps):\n",
        "  print(\"\\n\", idx, text)\n",
        "  mel_path = join(\"../Tacotron-2\", mel)\n",
        "  c = np.load(mel_path)\n",
        "  if c.shape[1] != hparams.num_mels:\n",
        "    np.swapaxes(c, 0, 1)\n",
        "  # Range [0, 4] was used for training Tacotron2 but WaveNet vocoder assumes [0, 1]\n",
        "  c = np.interp(c, (0, 4), (0, 1))\n",
        " \n",
        "  # Generate\n",
        "  waveform = wavegen(model, c=c, fast=True, tqdm=tqdm)\n",
        "  \n",
        "  waveforms.append(waveform)\n",
        "\n",
        "  # Audio\n",
        "  wave=IPython.display.display(Audio(waveform, rate=hparams.sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpGwx-cU6Uw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}